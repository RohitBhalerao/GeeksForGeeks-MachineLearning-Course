{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature names: ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
      "Target names: ['setosa' 'versicolor' 'virginica']\n",
      "\n",
      "Type of X is: <class 'numpy.ndarray'>\n",
      "\n",
      "First 5 rows of X:\n",
      " [[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]]\n"
     ]
    }
   ],
   "source": [
    "# load the iris dataset as an example \n",
    "from sklearn.datasets import load_iris \n",
    "iris = load_iris() \n",
    "\n",
    "# store the feature matrix (X) and response vector (y) \n",
    "X = iris.data \n",
    "y = iris.target \n",
    "\n",
    "# store the feature and target names \n",
    "feature_names = iris.feature_names \n",
    "target_names = iris.target_names \n",
    "\n",
    "# printing features and target names of our dataset \n",
    "print(\"Feature names:\", feature_names) \n",
    "print(\"Target names:\", target_names) \n",
    "\n",
    "# X and y are numpy arrays \n",
    "print(\"\\nType of X is:\", type(X)) \n",
    "\n",
    "# printing first 5 input rows \n",
    "print(\"\\nFirst 5 rows of X:\\n\", X[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (14, 5)\n",
      "\n",
      "Features: Index(['Outlook', 'Temperature', 'Humidity', 'Windy', 'Play'], dtype='object')\n",
      "\n",
      "Feature matrix:\n",
      "     Outlook Temperature Humidity  Windy\n",
      "0  overcast         hot     high  False\n",
      "1  overcast        cool   normal   True\n",
      "2  overcast        mild     high   True\n",
      "3  overcast         hot   normal  False\n",
      "4     rainy        mild     high  False\n",
      "\n",
      "Response vector:\n",
      " 0    yes\n",
      "1    yes\n",
      "2    yes\n",
      "3    yes\n",
      "4    yes\n",
      "Name: Play, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "# reading csv file \n",
    "data = pd.read_csv('weather.csv') \n",
    "\n",
    "# shape of dataset \n",
    "print(\"Shape:\", data.shape) \n",
    "\n",
    "# column names \n",
    "print(\"\\nFeatures:\", data.columns) \n",
    "\n",
    "# storing the feature matrix (X) and response vector (y) \n",
    "X = data[data.columns[:-1]] \n",
    "y = data[data.columns[-1]] \n",
    "\n",
    "# printing first 5 rows of feature matrix \n",
    "print(\"\\nFeature matrix:\\n\", X.head()) \n",
    "\n",
    "# printing first 5 values of response vector \n",
    "print(\"\\nResponse vector:\\n\", y.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90, 4)\n",
      "(60, 4)\n",
      "(90,)\n",
      "(60,)\n"
     ]
    }
   ],
   "source": [
    "# load the iris dataset as an example \n",
    "from sklearn.datasets import load_iris \n",
    "iris = load_iris() \n",
    "\n",
    "# store the feature matrix (X) and response vector (y) \n",
    "X = iris.data \n",
    "y = iris.target \n",
    "\n",
    "# splitting X and y into training and testing sets \n",
    "from sklearn.model_selection import train_test_split \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=1) \n",
    "\n",
    "# printing the shapes of the new X objects \n",
    "print(X_train.shape) \n",
    "print(X_test.shape) \n",
    "\n",
    "# printing the shapes of the new y objects \n",
    "print(y_train.shape) \n",
    "print(y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kNN model accuracy: 0.9833333333333333\n",
      "Predictions: ['versicolor', 'virginica']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rohitbhalerao/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['iris_knn.pkl']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the iris dataset as an example \n",
    "from sklearn.datasets import load_iris \n",
    "iris = load_iris() \n",
    "\n",
    "# store the feature matrix (X) and response vector (y) \n",
    "X = iris.data \n",
    "y = iris.target \n",
    "\n",
    "# splitting X and y into training and testing sets \n",
    "from sklearn.model_selection import train_test_split \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=1) \n",
    "\n",
    "# training the model on training set \n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "knn = KNeighborsClassifier(n_neighbors=3) \n",
    "knn.fit(X_train, y_train) \n",
    "\n",
    "# making predictions on the testing set \n",
    "y_pred = knn.predict(X_test) \n",
    "\n",
    "# comparing actual response values (y_test) with predicted response values (y_pred) \n",
    "from sklearn import metrics \n",
    "print(\"kNN model accuracy:\", metrics.accuracy_score(y_test, y_pred)) \n",
    "\n",
    "# making prediction for out of sample data \n",
    "sample = [[3, 5, 4, 2], [2, 3, 5, 4]] \n",
    "preds = knn.predict(sample) \n",
    "pred_species = [iris.target_names[p] for p in preds] \n",
    "print(\"Predictions:\", pred_species) \n",
    "\n",
    "# saving the model \n",
    "from sklearn.externals import joblib \n",
    "joblib.dump(knn, 'iris_knn.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Important points to note from the above code:\\n\\n    We create a knn classifier object using:\\n\\n    knn = KNeighborsClassifier(n_neighbors=3)\\n\\n    The classifier is trained using X_train data. The process is termed as fitting. We pass the feature matrix and the corresponding response vector.\\n\\n    knn.fit(X_train, y_train)\\n\\n    Now, we need to test our classifier on the X_test data. knn.predict method is used for this purpose. It returns the predicted response vector, y_pred.\\n\\n    y_pred = knn.predict(X_test)\\n\\n    Now, we are interested in finding the accuracy of our model by comparing y_test and y_pred. This is done using metrics module’s method accuracy_score:\\n\\n    print(metrics.accuracy_score(y_test, y_pred))\\n\\n    Consider the case when you want your model to make prediction on out of sample data. Then, the sample input can simply pe passed in the same way as we pass any feature matrix.\\n\\n    sample = [[3, 5, 4, 2], [2, 3, 5, 4]]\\n    preds = knn.predict(sample)\\n\\n    If you are not interested in training your classifier again and again and use the pre-trained classifier, one can save their classifier using joblib. All you need to do is:\\n\\n    joblib.dump(knn, 'iris_knn.pkl')\\n\\n    In case you want to load an already saved classifier, use the following method:\\n\\n    knn = joblib.load('iris_knn.pkl')\\n\\n\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Important points to note from the above code:\n",
    "\n",
    "    We create a knn classifier object using:\n",
    "\n",
    "    knn = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "    The classifier is trained using X_train data. The process is termed as fitting. We pass the feature matrix and the corresponding response vector.\n",
    "\n",
    "    knn.fit(X_train, y_train)\n",
    "\n",
    "    Now, we need to test our classifier on the X_test data. knn.predict method is used for this purpose. It returns the predicted response vector, y_pred.\n",
    "\n",
    "    y_pred = knn.predict(X_test)\n",
    "\n",
    "    Now, we are interested in finding the accuracy of our model by comparing y_test and y_pred. This is done using metrics module’s method accuracy_score:\n",
    "\n",
    "    print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "    Consider the case when you want your model to make prediction on out of sample data. Then, the sample input can simply pe passed in the same way as we pass any feature matrix.\n",
    "\n",
    "    sample = [[3, 5, 4, 2], [2, 3, 5, 4]]\n",
    "    preds = knn.predict(sample)\n",
    "\n",
    "    If you are not interested in training your classifier again and again and use the pre-trained classifier, one can save their classifier using joblib. All you need to do is:\n",
    "\n",
    "    joblib.dump(knn, 'iris_knn.pkl')\n",
    "\n",
    "    In case you want to load an already saved classifier, use the following method:\n",
    "\n",
    "    knn = joblib.load('iris_knn.pkl')\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
